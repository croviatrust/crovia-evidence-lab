Skip to main content
Cornell University
We gratefully acknowledge support from the Simons Foundation, member
institutions, and all contributors. Donate
 
arxiv logo > cs > arXiv:2307.09288
[                    ]

Help | Advanced Search

[All fields        ]
Search
arXiv logo
Cornell University Logo
[                    ] GO
quick links

  • Login
  • Help Pages
  • About

Computer Science > Computation and Language

arXiv:2307.09288 (cs)
[Submitted on 18 Jul 2023 (v1), last revised 19 Jul 2023 (this version, v2)]

Title:Llama 2: Open Foundation and Fine-Tuned Chat Models

Authors:Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,
Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril,
Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor
Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy
Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric
Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,
Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert
Stojnic, Sergey Edunov, Thomas Scialom
View a PDF of the paper titled Llama 2: Open Foundation and Fine-Tuned Chat
Models, by Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and
Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and
Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and
Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and
Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and
Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and
Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa
and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne
Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and
Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor
Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi
Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael
Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross
Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and
Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan
Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas
Scialom
View PDF

    Abstract:In this work, we develop and release Llama 2, a collection of
    pretrained and fine-tuned large language models (LLMs) ranging in scale
    from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama
    2-Chat, are optimized for dialogue use cases. Our models outperform
    open-source chat models on most benchmarks we tested, and based on our
    human evaluations for helpfulness and safety, may be a suitable substitute
    for closed-source models. We provide a detailed description of our approach
    to fine-tuning and safety improvements of Llama 2-Chat in order to enable
    the community to build on our work and contribute to the responsible
    development of LLMs.

Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI)
Cite as:  arXiv:2307.09288 [cs.CL]
          (or arXiv:2307.09288v2 [cs.CL] for this version)
          https://doi.org/10.48550/arXiv.2307.09288
          Focus to learn more
          arXiv-issued DOI via DataCite

Submission history

From: Thomas Scialom [view email]
[v1] Tue, 18 Jul 2023 14:31:57 UTC (17,722 KB)
[v2] Wed, 19 Jul 2023 17:08:59 UTC (17,722 KB)
Full-text links:

Access Paper:

    View a PDF of the paper titled Llama 2: Open Foundation and Fine-Tuned Chat
    Models, by Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert
    and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya
    Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas
    Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and
    David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian
    Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony
    Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas
    and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev
    and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya
    Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet
    and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and
    Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi
    and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan
    Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina
    Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov
    and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and
    Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom
  • View PDF
  • TeX Source

view license
Current browse context:
cs.CL
< prev   |   next >
new | recent | 2023-07
Change to browse by:
cs
cs.AI

References & Citations

  • NASA ADS
  • Google Scholar
  • Semantic Scholar

3 blog links

(what is this?)
export BibTeX citation Loading...

BibTeX formatted citation

×
[loading...          ]
Data provided by:

Bookmark

BibSonomy logo Reddit logo
(*) Bibliographic Tools

Bibliographic and Citation Tools

[ ] Bibliographic Explorer Toggle
Bibliographic Explorer (What is the Explorer?)
[ ] Connected Papers Toggle
Connected Papers (What is Connected Papers?)
[ ] Litmaps Toggle
Litmaps (What is Litmaps?)
[ ] scite.ai Toggle
scite Smart Citations (What are Smart Citations?)
( ) Code, Data, Media

Code, Data and Media Associated with this Article

[ ] alphaXiv Toggle
alphaXiv (What is alphaXiv?)
[ ] Links to Code Toggle
CatalyzeX Code Finder for Papers (What is CatalyzeX?)
[ ] DagsHub Toggle
DagsHub (What is DagsHub?)
[ ] GotitPub Toggle
Gotit.pub (What is GotitPub?)
[ ] Huggingface Toggle
Hugging Face (What is Huggingface?)
[ ] Links to Code Toggle
Papers with Code (What is Papers with Code?)
[ ] ScienceCast Toggle
ScienceCast (What is ScienceCast?)
( ) Demos

Demos

[ ] Replicate Toggle
Replicate (What is Replicate?)
[ ] Spaces Toggle
Hugging Face Spaces (What is Spaces?)
[ ] Spaces Toggle
TXYZ.AI (What is TXYZ.AI?)
( ) Related Papers

Recommenders and Search Tools

[ ] Link to Influence Flower
Influence Flower (What are Influence Flowers?)
[ ] Core recommender toggle
CORE Recommender (What is CORE?)

  • Author
  • Venue
  • Institution
  • Topic

( ) About arXivLabs

arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new
arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and
accepted our values of openness, community, excellence, and user data privacy.
arXiv is committed to these values and only works with partners that adhere to
them.

Have an idea for a project that will add value for arXiv's community? Learn
more about arXivLabs.

Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)

  • About
  • Help

  • Click here to contact arXiv Contact
  • Click here to subscribe Subscribe

  • Copyright
  • Privacy Policy

  • Web Accessibility Assistance
  • arXiv Operational Status

